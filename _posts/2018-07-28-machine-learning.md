---
layout: post
title: "机器学习入门（一）"
date: 2018-07-28 22:00:00
tags: ML
description: 梯度下降（Gradient Descent） 
---

# 机器学习是啥
## 监督学习
让我们来想象这样一个场景，现在有一些关于西瓜的信息，包括大小，重量，通过拍打所听到的声音，西瓜藤是否粗，是否是好瓜等。我们需要建立一个识别好瓜的模型，在给定了大小，重量等信息后判断其是否为好瓜。

在这个例子中，大小，重量等表示了西瓜的特征，是否是好瓜是“标签”，监督学习的特点是每一组数据都具有“标签”。
## 机器学习都做了些什么
现阶段的机器学习主要以监督学习为主。大多数监督学习都需要拟合一个目标函数，这个函数的大致框架已经被确定下来，我们要做的就是根据手中的数据来确定模型中的一些参数。所以常规的监督学习可以分为以下几步：
1. 确定目标函数，参数
2. 确定损失函数
3. 选择一个优化算法
4. 训练迭代至损失函数收敛

## 哪一些是我们需要做的
许多机器学习模块高度集成了许多已经被开发好的模型算法，而且配置好了我们需要确定的参数，于是我们只要提供数据给模型即可。

当然，训练完的结果也许并不能让你满意，你要做的就是根据一些调参手段进行对参数的优化，进而使模型在测试集上表现的更好

# 损失函数（Cost Function）
此处我用最常见的线性模型来解释, 目标函数如下

$$\hat{y} = \theta^{T}X + b \quad \quad \theta\in R^{n}$$

损失函数:

$$J(\theta) = \frac{1}{2m}\sum^{m}_{i=1}(\hat{y}^{(i)} - y^{(i)})^2$$

其中&nbsp;$\theta$&nbsp;表示参数矩阵，$\hat{y}^{(i)}$&nbsp;表示当前目标函数第$i$个值，$y^{(i)}$是第$i$个实际值。

> 损失函数还有很多种形式，比如LogisticRegression（逻辑回归）中所使用的形式：

$$J(\theta) = -\frac{1}{2m}\sum^{m}_{i=1}[y\log(\hat{y}^{(i)}) + (1 - y)\log(1 - \hat{y}^{(i)})]$$

> 以及推广形式（softmax）等原理相似故不在此举例

# 梯度下降
梯度下降的思想是沿着最快下降的方向进行对参数的修正，因为梯度所指的方向是指向最优解的方向所以取名为梯度下降，过程如下图：

![]({{ "/assets/images/gradientDescent.jpg" | relative_url }})
> 梯度的简单定义：对于一个n元函数$F(x_{1}, x_{2},..., x_{n})$的各变量的一阶偏导数所组成的向量，即

$$\triangledown F = (\frac{\partial F}{\partial x_{1}}, \frac{\partial F}{\partial x_{2}},..., \frac{\partial F}{\partial x_{n}})
$$

了解梯度后，我们再来看一下梯度下降如何作用于参数调整

> $for$&nbsp;&nbsp;$i$&nbsp;&nbsp;$to$&nbsp;&nbsp;$m:$
> $\quad \theta_{1} := \theta_{1} - \alpha \frac{\partial F}{\partial \theta_{1}}$
> $\quad ...$
> $\quad \theta_{i} := \theta_{i} - \alpha \frac{\partial F}{\partial \theta_{i}}$
> $\quad ...$

其中&nbsp;$:=$&nbsp;表示赋值，&nbsp;$\alpha$&nbsp;表示学习率，即走向极值时每一步的大小，这也是一个需要手动调整的参数

每次调整完了的参数&nbsp;$\theta$&nbsp;作用于&nbsp;$\hat{y}$&nbsp;使损失函数趋向于极值最后收敛。

